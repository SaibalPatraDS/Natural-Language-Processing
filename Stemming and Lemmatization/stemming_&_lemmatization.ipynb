{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c5de38-70c3-41d9-8b85-98aa26c859e7",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b75800-cee1-4891-a50f-9fa31758caae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ebd8ed1-ed0c-4921-b475-34eb8fbf5fab",
   "metadata": {},
   "source": [
    "**--> Both used to derive base word**\n",
    "\n",
    "**1. Stemming - Use some rules, some fixed rules like removing `ing`, removing `able` etc. to transform the word to its base word.**\n",
    "\n",
    "**2. Lemmatization - Use knowledge of the language to derive the base word, (linguistic knowledge)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024142c6-845d-4d9a-9618-e6ab7df91fbf",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340f4864-4f88-406d-a3bd-640d4c5f1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532b7b1-2f88-4898-8deb-4d43e883acb4",
   "metadata": {},
   "source": [
    "### Stemming in NLTK using `PorterStemmer` and `SnowballStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d80cc35-aa9b-48e3-b51a-2d6b3d338e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "## creating objects of the stemming classes\n",
    "stemmer_p = PorterStemmer()\n",
    "stemmer_s = SnowballStemmer(language = 'english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94717dc6-0e08-427f-ab52-4a2cbbba5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(nltk.stem.SnowballStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "230b4e91-77d9-49a1-9985-2babd8aabad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  |  Stemmer_P  |  Stemmer_S\n",
      "eating  |  eat  |  eat\n",
      "ate  |  ate  |  ate\n",
      "eats  |  eat  |  eat\n",
      "eat  |  eat  |  eat\n",
      "ability  |  abil  |  abil\n",
      "enjoying  |  enjoy  |  enjoy\n",
      "enjoyed  |  enjoy  |  enjoy\n",
      "funny  |  funni  |  funni\n",
      "meeting  |  meet  |  meet\n",
      "gone  |  gone  |  gone\n",
      "came  |  came  |  came\n"
     ]
    }
   ],
   "source": [
    "## let's apply stemming on some of the basis words\n",
    "\n",
    "words = ['eating', 'ate', 'eats', 'eat', 'ability', 'enjoying', 'enjoyed', 'funny', 'meeting', 'gone', 'came']\n",
    "\n",
    "print(\"Word\", \" | \", \"Stemmer_P\", \" | \", \"Stemmer_S\")\n",
    "\n",
    "for word in words:\n",
    "    print(word, \" | \", stemmer_p.stem(word), \" | \", stemmer_s.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a965538-5baf-4632-91aa-cb099a9ccff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a9c4e1e-9b96-4654-b2fd-3c7ebaf3252d",
   "metadata": {},
   "source": [
    "### Lemmatization using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b21272-a3a9-4af0-8535-f87f916418f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") #sm - small, md - medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4f85a8-8cd7-4872-8ed4-e96e881f6f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat  |  9837207709914848172\n",
      "ate  |  eat  |  9837207709914848172\n",
      "eats  |  eat  |  9837207709914848172\n",
      "eaten  |  eat  |  9837207709914848172\n",
      "ability  |  ability  |  11565809527369121409\n",
      "enjoying  |  enjoy  |  13716726989081948958\n",
      "enjoyed  |  enjoy  |  13716726989081948958\n",
      "funny  |  funny  |  4122592809438271869\n",
      "meeting  |  meeting  |  14798207169164081740\n",
      "gone  |  go  |  8004577259940138793\n",
      "came  |  come  |  5307304325359566725\n",
      "better  |  well  |  4525988469032889948\n"
     ]
    }
   ],
   "source": [
    "## application of lemmatization in spacy\n",
    "\n",
    "docs = nlp('eating ate eats eaten ability enjoying enjoyed funny meeting gone came better')\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc, \" | \", doc.lemma_, \" | \", doc.lemma)  #.lemma will o/p the hash of each word, unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef0ee5e-66b6-48a4-91cd-27cd49f1e4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\saiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##lemmatization in mltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "'''Wordnet is a large, free, and publicly available lexical database for the English language \n",
    "   aiming to establish structured semantic relationships between words.'''\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef0d6829-c6c6-4ad6-915c-2995b5486367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "ate  |  eat\n",
      "eats  |  eat\n",
      "eaten  |  eat\n",
      "ability  |  ability\n",
      "enjoying  |  enjoy\n",
      "enjoyed  |  enjoy\n",
      "funny  |  funny\n",
      "meeting  |  meet\n",
      "gone  |  go\n",
      "came  |  come\n",
      "hardly  |  hardly\n",
      "bought  |  buy\n",
      "better  |  better\n"
     ]
    }
   ],
   "source": [
    "## initialize wordnet lemmatizer\n",
    "wordnet_lemma = WordNetLemmatizer()\n",
    "\n",
    "words = ['eating', 'ate', 'eats', 'eaten', 'ability', 'enjoying', 'enjoyed', 'funny', 'meeting', 'gone', 'came', 'hardly', 'bought', 'better']\n",
    "\n",
    "for token in words:\n",
    "    print(token, \" | \", wordnet_lemma.lemmatize(token, pos = 'v'))\n",
    "    '''The code then performs lemmatization on each word in the list using the lemmatize() method of the WordNetLemmatizer class, \n",
    "    with the optional argument pos=\"v\" indicating that the words should be lemmatized as verbs.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c945d3-a460-4525-9424-a9a4904e132a",
   "metadata": {},
   "source": [
    "### Customizing the `attribute_ruler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "886b0bc9-9971-4c68-8939-df9a797c1d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa88b22c-35c6-44ae-9905-68784bbd6c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro  |  Brother\n",
      ",  |  ,\n",
      "do  |  do\n",
      "you  |  you\n",
      "wanna  |  wanna\n",
      "go  |  go\n",
      "?  |  ?\n",
      "Brah  |  Brother\n",
      ",  |  ,\n",
      "do  |  do\n",
      "n't  |  not\n",
      "say  |  say\n",
      "no  |  no\n",
      "!  |  !\n",
      "I  |  I\n",
      "am  |  be\n",
      "exhausted  |  exhaust\n",
      ".  |  .\n"
     ]
    }
   ],
   "source": [
    "## customizing the pipe\n",
    "cr = nlp.get_pipe('attribute_ruler')  \n",
    "\n",
    "cr.add([[{'TEXT':'Bro'}],[{'Text' : 'Brah'}]], {\"LEMMA\" : \"Brother\"})   #customizing the word bro and brah and will give brother whenever asked for lemma\n",
    "'''for Bro and Brah base word will be Brother.'''\n",
    "\n",
    "doc = nlp(\"Bro, do you wanna go? Brah, don't say no! I am exhausted.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4430c8e0-b3aa-4ba3-965c-27b1f1d409cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bro, 'Brother', 4347558510128575363)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0], doc[0].lemma_, doc[0].lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ebcb6-5a14-4e98-a099-27ac27124c48",
   "metadata": {},
   "source": [
    "## Practice\n",
    "----------------------\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcfd32e-4fed-4a0d-98c7-059cb0132bfe",
   "metadata": {},
   "source": [
    "### 1 . import necessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85a69116-68e5-4720-9fa7-86cf167c1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "## importing libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b22d546-ce0d-4324-b64a-dc8519dbce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading english language package\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d312bb6-af3a-4529-ad1d-27fdc52a3ce5",
   "metadata": {},
   "source": [
    "#### Exercise 1 :\n",
    "\n",
    "    1. Convert these list of words into base form using Stemming and Lemmatization and observe the transformations\n",
    "    2. Write a short note on the words that have different base words using stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cae29446-49d1-4004-b50d-76c494909e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## craeting instance of Stemmer classes\n",
    "stemmer_p = PorterStemmer()\n",
    "stemmer_s = SnowballStemmer(language = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "947907a7-8f3a-41c2-b5a6-d53c35611e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD  |  PorterStemmed_WORD  |  SNOWBALLStremmed_WORD\n",
      "running  |  run  |  run\n",
      "painting  |  paint  |  paint\n",
      "walking  |  walk  |  walk\n",
      "dressing  |  dress  |  dress\n",
      "likely  |  like  |  like\n",
      "children  |  children  |  children\n",
      "whom  |  whom  |  whom\n",
      "good  |  good  |  good\n",
      "ate  |  ate  |  ate\n",
      "fishing  |  fish  |  fish\n"
     ]
    }
   ],
   "source": [
    "#using stemming in nltk\n",
    "list_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']\n",
    "\n",
    "print(\"WORD\", \" | \", \"PorterStemmed_WORD\", \" | \", \"SNOWBALLStremmed_WORD\")\n",
    "\n",
    "for token in list_words:\n",
    "    print(token, \" | \", stemmer_p.stem(token), \" | \", stemmer_s.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed70261d-c583-4306-8460-cd5ba33a3559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using lemmatization in spacy\n",
    "\n",
    "doc = nlp(\"running painting walking dressing likely children who good ate fishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9882ca5a-09c3-4fc2-9ec3-00c915e70e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  |  run\n",
      "painting  |  paint\n",
      "walking  |  walk\n",
      "dressing  |  dress\n",
      "likely  |  likely\n",
      "children  |  child\n",
      "who  |  who\n",
      "good  |  good\n",
      "ate  |  eat\n",
      "fishing  |  fishing\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153714d-975e-4970-bd17-1e97b53f45ff",
   "metadata": {},
   "source": [
    "**<u>Conclusion</u> :**\n",
    "\n",
    " 1. Stemming is basically deriving base word of a word using some rules without knowledge of the language. For an example, `ate` is not converted to `eat` while using `stemmer` but `lemmatizer` has converted it to `eat`, cause it has the linguistic knowkedge.\n",
    " \n",
    " 2. On the other hand, `children` is converted to `child` by lemmatizer but it wasn't changed by stemmer.\n",
    " \n",
    " 3. Same happened with, `fishing`, where `stemming` has changed it to `fish` but lemmatization hasn't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f2b25-65ef-48b1-a21a-ff49ac1f6b2c",
   "metadata": {},
   "source": [
    "#### Exercise2:\n",
    "\n",
    "    convert the given text into it's base form using both stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44f5d5b0-edee-4977-b9e9-ff938232320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Latha is very multi talented girl.She is good at many skills like dancing, running, singing, playing.She also likes eating Pav Bhagi. she has a \n",
    "habit of fishing and swimming too.Besides all this, she is a wonderful at cooking too.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "166cd0d7-559b-4ccf-8550-3c107ea99aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latha is veri multi talent girl.sh is good at mani skill like danc , run , sing , playing.sh also like eat pav bhagi . she ha a habit of fish and swim too.besid all thi , she is a wonder at cook too .\n"
     ]
    }
   ],
   "source": [
    "##using stemming in nltk\n",
    "\n",
    "##step - 1 : word tokenization\n",
    "list_words = word_tokenize(text)\n",
    "\n",
    "stemmed_words = []\n",
    "\n",
    "## step - 2 : getting base form for each token using stemmer\n",
    "for token in list_words:\n",
    "    # print(token, \" | \", stemmer_p.stem(token), \" | \", stemmer_s.stem(token))\n",
    "    stemmed_words.append(stemmer_p.stem(token))\n",
    "\n",
    "    \n",
    "#step3: joining all words in a list into string using 'join()'\n",
    "final_text_stemming = ' '.join(stemmed_words)\n",
    "print(final_text_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46af1542-1db3-4857-99b5-65a1e4360435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latha be very multi talented girl . she be good at many skill like dancing , running , singing , play . she also like eat Pav Bhagi . she have a \n",
      " habit of fishing and swim too . besides all this , she be a wonderful at cook too . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using lemmatisation in spacy\n",
    "\n",
    "\n",
    "#step1: Creating the object for the given text\n",
    "doc = nlp(text)\n",
    "\n",
    "list_words = []\n",
    "#step2: getting the base form for each token using spacy 'lemma_'\n",
    "for token in doc:\n",
    "    # print(token, \" | \", token.lemma_)\n",
    "    list_words.append(token.lemma_)\n",
    "    \n",
    "\n",
    "#step3: joining all words in a list into string using 'join()'\n",
    "final_text_lemma = ' '.join(list_words)\n",
    "print(final_text_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d298dcc2-bc0f-4a08-81fb-3d17b6369d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latha be very multi talented girl.She be good at many skills like dance , run , sing , playing.She also like eat Pav Bhagi . she have a habit of fish and swim too.Besides all this , she be a wonderful at cook too .\n"
     ]
    }
   ],
   "source": [
    "## lemmatization using nltk\n",
    "\n",
    "## step - 1 : creating word tokenize\n",
    "list_words = word_tokenize(text)\n",
    "lemmatized_words = []\n",
    "\n",
    "## step - 2 : lemmatization using WordNetLemmatizer Class\n",
    "word_net = WordNetLemmatizer()\n",
    "for token in list_words:\n",
    "    # print(token, \" | \", word_net.lemmatize(token, pos = 'v'))\n",
    "    lemmatized_words.append(word_net.lemmatize(token, pos = 'v'))\n",
    "    \n",
    "    \n",
    "## step - 3 : joining all words in a list into string using `join()`\n",
    "final_text_lemmatize_nltk = ' '.join(lemmatized_words)\n",
    "print(final_text_lemmatize_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe97a6e-054f-421e-90c3-728f2ca8f48c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
